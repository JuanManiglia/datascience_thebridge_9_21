{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "En este notebook vas a ver un ejemplo de los procesos necesarios para realizar un análisis de sentimientos sobre Tweets. Para ello tendremos que seguir los siguientes pasos:\n",
    "1. Conseguir un Corpus: no es más que una base de datos de texto etiquetado\n",
    "2. Limpiar los datos\n",
    "3. Entrenar un modelo con el corpus\n",
    "4. Atacar a la API de Twitter\n",
    "5. Predecir los nuevos Tweets\n",
    "\n",
    "**Estos programas son muy útiles en campañas de marketing, para monitorizar el lanzamiento de un nuevo producto, realizar seguimiento en Twitter de eventos, o simplemente tener monitorizadas ciertas cuentas o hashtags para tener un programa de análisis real time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "Para conseguir el corpus tendremos que registrarnos en la [página del TASS](http://tass.sepln.org/tass_data/download.php), que se trata de una asociación de análisis semántico que encargada de recopilar texto y mantenerlo etiquetado. \n",
    "\n",
    "Para datasets en ingles lo tenemos más fácil ya que con librerías como [TextBlob](https://textblob.readthedocs.io/en/dev/) podemos predecir directamente la polaridad del Tweet, con modelos ya preentrenados. En el caso del castellano necesitamos acudir a un corpus etiquetado para entrenar nuestro modelo.\n",
    "\n",
    "Registrate en el TASS y accede a sus corpus a través de un link que te llegará al correo tras el registro.\n",
    "\n",
    "![imagen](img/tass_register.png)\n",
    "\n",
    "\n",
    "Una vez estes registrado, descárgate el corpus de tweets en español de entrenamiento. En este punto lo ideal es coger un corpus que se adapte lo máximo posible a los tipos de tweets que intentamos predecir, es decir, si queremos predecir tweets sobre política, procurar elegir un corpus que tenga vocabulario de política.\n",
    "\n",
    "En este notebook se va a elegir un corpus genérico con no demasiados registros para aligerar la limpieza y entrenamiento de los modelos.\n",
    "\n",
    "![imagen](img/download_train_spanish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict = {\n",
    "    'User': [],\n",
    "    'Content': [],\n",
    "    'Date': [],\n",
    "    'Lang': [],\n",
    "    'Polarity': [],\n",
    "    'Type': []\n",
    "}\n",
    "\n",
    "for i in root.iter('tweet'):\n",
    "    #### CODE ####\n",
    "    \n",
    "df = pd.DataFrame(raw_dict)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna de polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos los valores unicos de la columna de polaridad\n",
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columna de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de datos\n",
    "#### Polaridad\n",
    "Vamos a clasificar los Tweets como buenos o malos, por lo que haremos la siguiente agrupación de la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polaridad_fun(x):\n",
    "    if x in ('P', 'P+'):\n",
    "        return 0\n",
    "    elif x in ('N', 'N+'):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos cargamos los NONE y los neutros\n",
    "#### CODE ####\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos la columna a 1s y 0s. Y el tipo\n",
    "#### CODE ####\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idioma\n",
    "Nos quedamos con los tweets en español. Si no tuviésemos esa columna podríamos acudir a librerías como `langid` o `langdetect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los tweets en español\n",
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos con cuantos registros nos hemos quedado despues del filtrado\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los duplicados\n",
    "#### CODE ####\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signos de puntuación\n",
    "Eliminamos signos de puntuación: puntos, comas, interrogaciones, paréntesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "\n",
    "def signs_tweets(tweet):\n",
    "    return signos.sub('', tweet.lower())\n",
    "\n",
    "df['Content'] = df['Content'].apply(signs_tweets)\n",
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(df):\n",
    "    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "\n",
    "df['Content'] = df['Content'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros\n",
    "Podríamos hacer un preprocesado mucho más fino:\n",
    "1. Hashtags\n",
    "2. Menciones\n",
    "3. Abreviaturas\n",
    "4. Faltas de ortografía\n",
    "5. Risas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "Para montar el modelo tendremos que seguir los siguientes pasos\n",
    "1. Eliminamos las stopwords\n",
    "2. Aplicamos un stemmer, SnowBall por ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    return \" \".join([word for word in df.split() if word not in spanish_stopwords])\n",
    "\n",
    "#### CODE ####\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos columnas\n",
    "Nos quedamos con las columnas que nos interesan para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Content', 'Polarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montamos Pipeline\n",
    "Modelos que suelen funcionar bien con pocas observaciones y muchas features son la Regresión logística el LinearSVC o Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####\n",
    "\n",
    "# Aqui definimos el espacio de parámetros a explorar\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best acc:\", grid_search.best_score_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('finished_model.model', \"wb\") as archivo_salida:\n",
    "    pickle.dump(grid_search.best_estimator_, archivo_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicciones\n",
    "#### API de Twitter\n",
    "Lo primero que tenemos que hacer es conseguir nuevos Tweets. Para ello:\n",
    "1. Nos registramos en la [web de desarrolladores de Twitter](https://developer.twitter.com/en/apply-for-access)\n",
    "2. Bajamos el paquete `tweepy` para atacara  la API de Twitter\n",
    "3. Buscamos un Hashtag de tendencia\n",
    "4. Nos logamos y monitorizamos el hastag\n",
    "5. Aplicamos la limpieza a los Tweets\n",
    "6. Predecimos la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy  \n",
    "import time\n",
    "import csv\n",
    "\n",
    "import json\n",
    "\n",
    "with open('./credentials.json') as f:\n",
    "    credentials = json.load(f)\n",
    "    \n",
    "    \n",
    "# Credenciales de la web de desarroladores\n",
    "access_token = credentials['access_token']  \n",
    "access_token_secret = credentials['access_token_secret']  \n",
    "consumer_key = credentials['consumer_key']\n",
    "consumer_secret = credentials['consumer_secret'] \n",
    "\n",
    "# Nos autenticamos en la API\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)  \n",
    "    auth.set_access_token(access_token, access_token_secret)  \n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict = {\n",
    "    'author_name': [],\n",
    "    'created_at': [],\n",
    "    'content': [],\n",
    "    'author_description': [],\n",
    "    'author_followers_count': [],\n",
    "    'author_profile_image_url': [],\n",
    "    'author_location': [],\n",
    "    'author_profile_background_image_url': [],\n",
    "    'author_notifications': [],\n",
    "    'geo': [],\n",
    "    'coordinates': [],\n",
    "    'entities': [],\n",
    "    'place': []\n",
    "}\n",
    "\n",
    "# https://docs.tweepy.org/en/latest/api.html#search-methods\n",
    "\n",
    "#### CODE ####\n",
    "\n",
    "for tweet in cursor:\n",
    "    \n",
    "    raw_dict['author_name'].append(tweet.author.name)\n",
    "    raw_dict['created_at'].append(tweet.created_at)\n",
    "    raw_dict['content'].append(tweet.text)\n",
    "    raw_dict['author_description'].append(tweet.author.description)\n",
    "    raw_dict['author_followers_count'].append(tweet.author.followers_count)\n",
    "    raw_dict['author_profile_image_url'].append(tweet.author.profile_image_url)\n",
    "    raw_dict['author_location'].append(tweet.author.location)\n",
    "    raw_dict['author_profile_background_image_url'].append(tweet.author.profile_background_image_url)\n",
    "    raw_dict['author_notifications'].append(tweet.author.notifications)\n",
    "    raw_dict['geo'].append(tweet.geo)\n",
    "    raw_dict['coordinates'].append(tweet.coordinates)\n",
    "    raw_dict['entities'].append(tweet.entities)\n",
    "    raw_dict['place'].append(tweet.place)\n",
    "    \n",
    "\n",
    "test = pd.DataFrame(raw_dict)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "print(test.author_profile_image_url[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = test.copy()\n",
    "\n",
    "# Seleccion de columnas\n",
    "#### CODE ####\n",
    "\n",
    "# Eliminar duplicados\n",
    "#### CODE ####\n",
    "\n",
    "# Signos de puntuacion\n",
    "#### CODE ####\n",
    "\n",
    "# Eliminamos links\n",
    "#### CODE ####\n",
    "\n",
    "# Nos cargamos stopwords\n",
    "#### CODE ####\n",
    "\n",
    "# Aplicamos el Stemmer\n",
    "#### CODE ####\n",
    "test_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos el pipeline con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finished_model.model', \"rb\") as archivo_entrada:\n",
    "    pipeline_importada = pickle.load(archivo_entrada)\n",
    "    \n",
    "print(pipeline_importada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE ####\n",
    "\n",
    "test_clean['Polarity'] = pd.Series(predictions)\n",
    "for i in test_clean[:20].iterrows():\n",
    "    print(i[1]['Polarity'])\n",
    "    print(i[1]['content'])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
